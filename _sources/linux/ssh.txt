SSH
===

* http://support.suso.com/supki/SSH_Tutorial_for_Linux
*
https://www.digitalocean.com/community/tutorials/how-to-use-ssh-to-connect-to-a-remote-server-in-ubuntu
* http://www.openbsd.org/papers/auug2002-ssh.pdf



adding ather users keys
----------------------------------------------------------------------

put all keys into the authorized_keys file and distribute them on all vms by placing them at ~/.ssh/authorized_keys

WARNING WARNING: before you do this you must make sure that all other keys such as the once created on the vm into the user account or the cc account are also copied into that file. If you just replace the file you may lose access if you forgot to include the appropriate keys, you may not recover form this if you overwrote the wrong keys.

security groups
----------------------------------------------------------------------

we have seen many students deleting and modifying the default security group, thus we recommend each project sets up a clearly distingishable security group

problem is I am not entirely sure what you want and need to do as this is for sure project dependent. Here an attemt on an FAQ

in the following i use <protalname> as portalname of futuresystems.

security groups**: as projects on kilo and chameleon are shared it is
best to create your own security group under the na
----------------------------------------------------------------------

	<portalname>-default

and add your rules to that

naming of vms*
----------------------------------------------------------------------

	all vms should have your <portalname> in the name. This will allow you to identify the vms you work with easily

naming on key on the cloud
----------------------------------------------------------------------

	it is best to name the key on the cloud with your <portalname> in order not to confuse that with others its also good practice to optionally put  -key behind it, SO your key name would be <portalname>-key

accessing root on chameleon images
----------------------------------------------------------------------

	as documented in the chameleon documentation of their openstack cloud the root user on chameleon images is cc and not ubuntu

unsetting OS variabples
----------------------------------------------------------------------

	if you used or sourced openrc.sh files from kilo and switch to chameleon, you natuarally need to unset them. Best is to make sure your do not have tham in bashrc or bash profile and double check with env that no variable starts with OS_. If so you need to unset and best start a new terminal, doublecheck

keys on multiple machines
----------------------------------------------------------------------

please be aware that you may work with keys from multiple machines. firts you typically have the id_rsa key on your laptop, that is the key that you should upload with the openstack command to the cloud under the name <portalname>-key this name you will use when booting up machines from your laptop. 
However, as this key does not exists on other machines other than your laptop (and if it does you violate any security) You will have to add any other public key to the authorized_keys file in your .ssh directory

The way to do this is to login for example in all your vms with a script from your laptop, invoke the ssh-keygen there and copy all the keys to your local machine while concatenating them with cat >> to a file ~/authorized_keys. Than you copy that file with all the keys (dont forget to add the authorized keys file from your laptop) to all machines. into ~/.ssh/authorized_keys

Now its time to test if this works, use ssh-add to add your local key to the keychain, than you do not have to always type in a password going to a vm. Remember that the ssh-keygen on the vms will not use a password. This will than allow that you can login from each vm to each vm.

SIMPLE enough, and i recommend writing a shellor even better a python script for that

tunnel
----------------------------------------------------------------------

at times you will be out of floating ips and you can assign one vm form your pool a floating ip or as badi has set up a “proxy” vm.

You can than add to the .ssh/config file the ssh tunnel specification to any of the machines. a script to do this is helpful.

For example

Host vm1
    User  gvonlasz
    Hostname <ipof my proxy machine
    ProxyCommand  ssh <ipofthemachineirealyneedtogetto> nc %h %p

so this way if i say 

ssh vm1 uname -a

	I do a tunnel hrough the proxy machine and execute on the vm1

	laptop -> proxy -> vm1

Seems simple enough

(hopefully i did not mistype here ;-) )

As you can see at no time I leave the laptop all controlled programmatically

connecting between vms
----------------------------------------------------------------------

assume you have done the key distribution, than it is possible to simply 

execute an ssh command to reach another vm as you are authorized to access it via your password less keys. For each vm you have your own key. As in the previous step for distribution you have only copied public keys this will all be secure.


Conclusion
----------------------------------------------------------------------

This sets up a really elementary security infrastructure so you can login from laptop to any of your vms and from any vm to each other. 


But my vm is active, I can not login
----------------------------------------------------------------------

Just because your vm is active does not mean you can login. remember a machine gets active whne it starts booting. the boot process can take multiple minutes

Denial of service attack to your own machine
----------------------------------------------------------------------

In the past I saw students looping through an ssh command and as soon as it failed issued a new one during boot. They did it so many times that they flodded the network as they did it not just with one but many many more vms. Multiply this by x users and you can see that alone through this process you can create a denial of service attack on cloud services. So what you have to do is to put a sleep between such ssh attemts to see if your vm is really up. Put at least 30 seconds. at time it can be as much as 10 minutes dependent on usage. 

Focussing to get a vm to run thats obviously dead
----------------------------------------------------------------------

sometimes I see effort being spend on reviving a vm as the argument is i spend so much time setting it up.

not so, if you spend so much time it should all be done in scripts and thus repeatable. Thus if something is wrong you often are better of just replicating the setup on a new vm and delete toe old one(s).


THIS IS A ROUGH DRAFT THAT I INTEND TO IMPROVE 

Also we will have this setup in the next class available as command

e.g.

cm cluster -n 3 

will do all of the above automatically


